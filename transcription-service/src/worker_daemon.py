"""
Transcription Worker Daemon

Continuously polls Redis for transcription jobs and processes them.
This replaces the one-shot CLI approach with a long-running worker.

Usage:
    python worker_daemon.py
"""
import asyncio
import json
import time
import signal
import sys
from pathlib import Path
import redis.asyncio as redis

# Add parent directory to path for imports
sys.path.insert(0, str(Path(__file__).parent))

from config import TranscriptionConfig
from core.diarization import apply_pytorch_patch
from core.processor import process_episode_async, load_history, save_history
from core.audio import TranscriptionWorker
from podcast_transcriber_shared.database import create_episode, update_episode_status, EpisodeStatus


# Global flag for graceful shutdown
shutdown_event = asyncio.Event()


async def main():
    """Main worker daemon loop."""
    
    # Apply PyTorch patch for Pyannote compatibility
    apply_pytorch_patch()
    
    # Load configuration
    config = TranscriptionConfig.from_env()
    
    # Setup signal handlers
    loop = asyncio.get_running_loop()
    try:
        loop.add_signal_handler(signal.SIGINT, lambda: shutdown_event.set())
        loop.add_signal_handler(signal.SIGTERM, lambda: shutdown_event.set())
    except NotImplementedError:
        signal.signal(signal.SIGINT, lambda s, f: shutdown_event.set())
        signal.signal(signal.SIGTERM, lambda s, f: shutdown_event.set())
    
    print("\n")
    print("‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó")
    print("‚ïë          Podcast Transcription Worker Daemon v2.0           ‚ïë")
    print("‚ïë          Redis Queue-Based Architecture                      ‚ïë")
    print("‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù")
    
    # Connect to Redis
    try:
        r = redis.from_url(config.redis_url, decode_responses=True)
        await r.ping()
        print(f"‚úÖ Redis connection established: {config.redis_url}")
    except Exception as e:
        print(f"‚ùå Failed to connect to Redis: {e}")
        sys.exit(1)

    # Initialize Persistent Transcription Worker (Heavy Model Load)
    print(f"\nüì¶ Initializing Transcription Worker (Loading Models)...")
    try:
        # Run worker initialization in executor to avoid blocking the loop
        worker = await loop.run_in_executor(
            None,
            lambda: TranscriptionWorker(
                whisper_model=config.whisper_model,
                device=config.device,
                compute_type=config.compute_type,
                batch_size=config.batch_size
            )
        )
        print("‚úÖ Worker initialized and models loaded")
    except Exception as e:
        print(f"‚ùå Failed to initialize worker: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)
    
    print(f"\nüîÑ Worker started in daemon mode")
    print(f"üìã Polling queue: 'transcription_queue'")
    print("=" * 64)
    
    job_count = 0
    history = load_history(config)
    
    while not shutdown_event.is_set():
        try:
            # Poll Redis queue
            result = await r.blpop('transcription_queue', timeout=5)
            
            if result:
                _, job_data = result
                job_count += 1
                
                print(f"\nüì• Job #{job_count} received from queue")
                
                episode_id = None
                try:
                    job_payload = json.loads(job_data)
                    
                    # Parse episode_id from simplified job payload
                    # New format: {"episode_id": "yt:video:123", "timestamp": "..."}
                    episode_id = job_payload.get('episode_id')
                    
                    # Backward compatibility for legacy format: {"episodes": ["yt:video:123"], ...}
                    if not episode_id and 'episodes' in job_payload:
                        episodes = job_payload.get('episodes', [])
                        if episodes and isinstance(episodes, list) and len(episodes) > 0:
                            episode_id = episodes[0]
                            print(f"‚ö†Ô∏è  Legacy job format detected. Extracted episode_id: {episode_id}")

                    if not episode_id:
                        print(f"‚ö†Ô∏è  Job contains no episode_id, skipping")
                        continue
                    
                    print(f"üìã Processing episode: {episode_id}")
                    
                    # Fetch full episode data from PostgreSQL (not SQLite)
                    from podcast_transcriber_shared.database import get_episode_by_id
                    episode = await get_episode_by_id(episode_id, load_transcript=False)
                    
                    if not episode:
                        print(f"‚ö†Ô∏è  Episode {episode_id} not found in database, skipping")
                        continue
                    
                    print(f"‚úÖ Retrieved episode from PostgreSQL: {episode.title}")
                    
                    # Convert Episode object to dict for process_episode_async
                    episode_data = {
                        'id': episode.id,
                        'episode_title': episode.title,
                        'feed_title': episode.podcast_name,
                        'audio_url': episode.meta_data.get('audio_url') if episode.meta_data else None,
                        'published_date': episode.meta_data.get('published_date') if episode.meta_data else None,
                    }
                    
                    # Update status to PROCESSING
                    await update_episode_status(episode_id, EpisodeStatus.PROCESSING)
                    print(f"üîÑ Updated status to PROCESSING")

                    # Process Episode (Async)
                    # Worker's process_episode_async will save transcript to PostgreSQL
                    success, episode_id = await process_episode_async(
                        episode_data=episode_data,
                        config=config,
                        history=history,
                        worker=worker,
                        from_queue=True
                    )
                    
                    if success:
                        save_history(config, history)
                        print(f"‚úÖ Episode completed successfully: {episode_id}")
                    else:
                        await update_episode_status(episode_id, EpisodeStatus.FAILED)
                        print(f"‚ö†Ô∏è  Episode processing failed: {episode_id}")
                    
                    print(f"‚úÖ Job #{job_count} completed successfully")
                    
                except json.JSONDecodeError:
                    print(f"‚ùå Invalid JSON in job data")
                except Exception as e:
                    print(f"‚ùå Error processing job: {e}")
                    import traceback
                    traceback.print_exc()
                    if episode_id:
                        await update_episode_status(episode_id, EpisodeStatus.FAILED)
                
                print("=" * 64)
            
        except redis.ConnectionError:
            print("‚ùå Redis connection lost, retrying...")
            await asyncio.sleep(5)
        except Exception as e:
            print(f"‚ùå Unexpected error: {e}")
            await asyncio.sleep(5)

    # Cleanup
    await r.close()
    del worker
    print("\nüõë Worker daemon shut down")


if __name__ == "__main__":
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        pass
