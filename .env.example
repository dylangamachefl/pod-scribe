# =================================================================
# Podcast Transcriber - Environment Configuration
# =================================================================
# Copy this file to .env and fill in your actual values
#
# IMPORTANT: Never commit .env to version control!
# The .env file contains sensitive credentials.
# =================================================================

# =================================================================
# AI/ML API Keys
# =================================================================

# Hugging Face Token (Required for speaker diarization)
# Get your token at: https://huggingface.co/settings/tokens
HUGGINGFACE_TOKEN=hf_your_token_here

# Google Gemini API Key (Required for summarization)
# Get your key at: https://makersuite.google.com/app/apikey
GEMINI_API_KEY=your_gemini_api_key_here

# =================================================================
# Transcription Service Configuration
# =================================================================

# Whisper Model (large-v2, medium, small, base, tiny)
WHISPER_MODEL=large-v2

# Device (cuda for GPU, cpu for CPU-only)
DEVICE=cuda

# Compute Type (int8, float16, float32)
# int8 is faster and uses less memory, float16 is more accurate
COMPUTE_TYPE=int8

# Batch Size (higher = faster but more memory)
BATCH_SIZE=4

# =================================================================
# Service URLs (for inter-service communication)
# =================================================================

# RAG Service URL (where RAG API is running)
# Default: http://localhost:8000 (host machine)
# Docker internal: http://rag-service:8000
RAG_SERVICE_URL=http://localhost:8000

# Summarization Service URL (where Summarization API is running)
# Default: http://localhost:8002 (host machine)
# Docker internal: http://summarization-service:8002
SUMMARIZATION_SERVICE_URL=http://localhost:8002

# Service Timeout (seconds to wait for service responses)
SERVICE_TIMEOUT=60

# Service Retry Attempts (number of times to retry failed service calls)
SERVICE_RETRY_ATTEMPTS=3

# Service Retry Delay (initial delay in seconds, uses exponential backoff)
SERVICE_RETRY_DELAY=2.0

# =================================================================
# Ollama Configuration (for RAG)
# =================================================================

# Ollama API URL
# Default (host): http://localhost:11434
# Docker: http://host.docker.internal:11434
OLLAMA_API_URL=http://host.docker.internal:11434

# Ollama Chat Model (for RAG Q&A)
OLLAMA_CHAT_MODEL=qwen3:rag

# Ollama Embedding Model (for RAG vector search)
OLLAMA_EMBED_MODEL=nomic-embed-text

# =================================================================
# Qdrant Configuration (Vector Database)
# =================================================================

# Qdrant URL
# Local: http://localhost:6333
# Docker: http://qdrant:6333
QDRANT_URL=http://qdrant:6333

# Qdrant Collection Name
QDRANT_COLLECTION_NAME=podcast_transcripts

# =================================================================
# RAG Service Configuration
# =================================================================

# Embedding Dimension (nomic-embed-text = 768)
EMBEDDING_DIMENSION=768

# Chunk Size (characters per chunk)
CHUNK_SIZE=500

# Chunk Overlap (characters)
CHUNK_OVERLAP=100

# Top K Results (number of chunks to retrieve)
TOP_K_RESULTS=5

# Similarity Threshold (minimum score for results)
SIMILARITY_THRESHOLD=0.7

# Hybrid Search Configuration
HYBRID_SEARCH_ENABLED=true
BM25_WEIGHT=0.5
QDRANT_WEIGHT=0.5
HYBRID_TOP_K=5

# =================================================================
# Redis Configuration (for event-driven architecture)
# =================================================================

# Redis URL for pub/sub messaging
# Default for Docker: redis://redis:6379
# Default for host: redis://localhost:6379
REDIS_URL=redis://redis:6379

# =================================================================
# API Ports
# =================================================================

# RAG Service Port
RAG_API_PORT=8000

# Transcription API Port
TRANSCRIPTION_API_PORT=8001

# Summarization Service Port
SUMMARIZATION_API_PORT=8002

# Frontend Port
FRONTEND_PORT=3000

# =================================================================
# Summarization Configuration
# =================================================================

# Gemini Model (gemini-2.5-flash-lite, gemini-pro, etc.)
SUMMARIZATION_MODEL=gemini-2.5-flash-lite

# Frontend URL (for CORS)
SUMMARIZATION_FRONTEND_URL=http://localhost:3000

# =================================================================
# RAG Frontend Configuration
# =================================================================

# Frontend URL (for CORS)
RAG_FRONTEND_URL=http://localhost:3000

# =================================================================
# Notes
# =================================================================
#
# 1. Service URLs:
#    - When running on HOST (transcription worker), use localhost
#    - When running in DOCKER, use service names or host.docker.internal
#
# 2. API Keys:
#    - GEMINI_API_KEY can also be stored in secrets/gemini_api_key.txt
#    - Docker Secrets are more secure than environment variables
#
# 3. GPU Requirements:
#    - Transcription service requires NVIDIA GPU with CUDA
#    - RAG service can run on CPU (Ollama handles embeddings)
#
# 4. Ollama Setup:
#    - Install from: https://ollama.ai/download
#    - Pull models: 
#      ollama pull qwen3:8b
#      ollama pull nomic-embed-text
#    - Create custom model:
#      ollama create qwen3:rag -f Modelfile
#    - See QUICKSTART.md for Modelfile content
#
# =================================================================
